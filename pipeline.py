# -*- coding: utf-8 -*-
"""Project_Deployment_Fewshot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1mSQAdUSv0ws6VIjqknYT_uCmJk73ri
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf

import sys
sys.path.append('/content/models')

import glob
import os
import numpy as np
import cv2
from ultralytics import YOLO
import json
from pathlib import Path
import logging
from typing import List, Union, Tuple, Optional
from time import sleep
from official.projects.movinet.modeling import movinet
from official.projects.movinet.modeling import movinet_model

from PIL import Image
import tempfile
import shutil
import gradio as gr

from google import genai
from google.genai import types

from collections import defaultdict

from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

import time

import traceback

import torch
import re
from sklearn.metrics.pairwise import cosine_similarity
from torchvision.models import resnet50
import torchvision.transforms as transforms
import pickle
import warnings

class TrafficLightDetector:
    def __init__(self, model_path='yolov8m.pt', confidence_threshold=0.5):
       self.model = YOLO(model_path)
       self.confidence_threshold = confidence_threshold


    def detect_in_sequence(self, sequence_full_path):
        """
        Analyze frames in a folder for traffic lights and return detection results.
        """
        all_responses = []

        if not os.path.exists(sequence_full_path):
            print(f"Skipping: {sequence_full_path} (Folder not found)")
            return ["Folder not found"]

        print(f"Processing: {sequence_full_path}")

        for frame_num in range(10):  # frame_0000 to frame_0009
            frame_path = os.path.join(sequence_full_path, f'frame_{frame_num:04d}.jpg')
            if not os.path.exists(frame_path):
                print(f"Missing frame: {frame_path}")
                all_responses.append(f'Frame_{frame_num:04d}: Missing frame')
                continue

            frame = cv2.imread(frame_path)
            if frame is None:
                print(f"Failed to read frame: {frame_path}")
                all_responses.append(f'Frame_{frame_num:04d}: Failed to read frame')
                continue

            try:
                results = self.model(frame)
                response = f'Frame_{frame_num:04d}: '
                detected = False

                for result in results:
                    for box, conf, cls_id in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
                        if cls_id == 9 and conf > self.confidence_threshold:  # Traffic light class
                            x1, y1, x2, y2 = map(int, box.tolist())
                            roi = frame[y1:y2, x1:x2]

                            if roi.size == 0:
                                print(f"Invalid ROI at frame {frame_num}, skipping...")
                                continue

                            color = self._get_traffic_light_color(roi)
                            response += f'Traffic Light ({x1},{y1},{x2},{y2}) {color} '
                            detected = True

                            # Optional: draw on frame (you can skip saving for now)
                            color_map = {"red": (0, 0, 255), "yellow": (0, 255, 255), "green": (0, 255, 0)}
                            cv2.rectangle(frame, (x1, y1), (x2, y2), color_map.get(color, (255, 255, 255)), 2)
                            cv2.putText(frame, f"Traffic Light: {color}", (x1, y1 - 10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_map.get(color, (255, 255, 255)), 2)

                if not detected:
                    response += 'No traffic light detected'

                all_responses.append(response)

            except Exception as e:
                print(f"Error processing frame {frame_path}: {e}")
                all_responses.append(f'Frame_{frame_num:04d}: Error - {str(e)}')

        return all_responses

    def _get_traffic_light_color(self, roi):
        """Detect traffic light color from region of interest (ROI)."""
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

        lower_red1, upper_red1 = np.array([0, 150, 100]), np.array([10, 255, 255])
        lower_red2, upper_red2 = np.array([170, 150, 100]), np.array([180, 255, 255])
        lower_yellow, upper_yellow = np.array([22, 150, 150]), np.array([38, 255, 255])
        lower_green, upper_green = np.array([40, 120, 120]), np.array([90, 255, 255])

        red_mask = cv2.bitwise_or(cv2.inRange(hsv, lower_red1, upper_red1),
                                  cv2.inRange(hsv, lower_red2, upper_red2))
        yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)
        green_mask = cv2.inRange(hsv, lower_green, upper_green)

        red_pixels = cv2.countNonZero(red_mask)
        yellow_pixels = cv2.countNonZero(yellow_mask)
        green_pixels = cv2.countNonZero(green_mask)

        max_pixels = max(red_pixels, yellow_pixels, green_pixels)
        if max_pixels == red_pixels:
            return "red"
        elif max_pixels == yellow_pixels:
            return "yellow"
        elif max_pixels == green_pixels:
            return "green"
        else:
            return "unknown"

    def filter_detections(self, frame_data):
        """
        Extract the most common color from each frame's description.
        """
        filtered = []
        for frame in frame_data:
            frame_num = frame.split(":")[0]
            most_common = self._get_most_common_color(frame)
            filtered.append(f"{frame_num}: {most_common}")
        return filtered

    def _get_most_common_color(self, frame_data):
        if "No traffic light detected" in frame_data:
            return "No traffic light"

        colors = []
        parts = frame_data.split("Traffic Light")
        for part in parts[1:]:
            if "red" in part:
                colors.append("red")
            elif "green" in part:
                colors.append("green")
            elif "yellow" in part:
                colors.append("yellow")

        if not colors:
            return "None"

        counts = {"red": colors.count("red"),
                  "green": colors.count("green"),
                  "yellow": colors.count("yellow")}
        return max(counts, key=counts.get)

# Initialize the detector
detector = TrafficLightDetector(confidence_threshold=0.5)

class TurnClassifier:
    def __init__(self, model_path):
        self.model_id = 'a2'
        self.backbone = movinet.Movinet(model_id=self.model_id)
        self.model = movinet_model.MovinetClassifier(backbone=self.backbone, num_classes=600)
        self.model.build([1, 1, 1, 1, 3])

        with tf.device('/device:GPU:0'):
            self.batch_size = 16
            self.num_frames = 10
            self.resolution = 224

            self.model = self.build_classifier(
                backbone=self.backbone,
                num_classes=3,
                dropout_rate=0.3,
                freeze_backbone=True
            )

            self.model.load_weights(model_path)

        self.class_names = [
            'The driver took a left turn',
            'The driver took a right turn',
            'The driver did not take a turn'
        ]

    def build_classifier(self, backbone, num_classes=3, dropout_rate=0.3, freeze_backbone=True):
        model = movinet_model.MovinetClassifier(
            backbone=backbone,
            num_classes=num_classes,
            dropout_rate=dropout_rate)

        model.build([self.batch_size, self.num_frames, self.resolution, self.resolution, 3])

        if freeze_backbone:
            total_backbone_layers = len(backbone.layers)
            for i, layer in enumerate(backbone.layers):
                if i < total_backbone_layers - 4:
                    layer.trainable = False
                else:
                    layer.trainable = True

        for layer in model.layers:
            if not isinstance(layer, tf.keras.layers.InputLayer) and not layer is backbone:
                layer.trainable = True

        return model

    def get_frame_sequence(self, sequence_dir):
        frame_paths = sorted(glob.glob(os.path.join(sequence_dir, '*.jpg')))
        return frame_paths

    def load_frame(self, frame_path, resolution):
        img = tf.io.read_file(frame_path)
        img = tf.image.decode_jpeg(img)
        img = tf.image.resize(img, (resolution, resolution))
        img = tf.ensure_shape(img, (resolution, resolution, 3))
        img = tf.cast(img, tf.float32) / 255.0
        return img

    def process_video_sequence(self, sequence_dir):
        frame_paths = self.get_frame_sequence(sequence_dir)

        if len(frame_paths) < self.num_frames:
            frame_paths = frame_paths + [frame_paths[-1]] * (self.num_frames - len(frame_paths))
        elif len(frame_paths) > self.num_frames:
            frame_paths = frame_paths[:self.num_frames]

        frames = []
        for frame_path in frame_paths:
            frame = self.load_frame(frame_path, self.resolution)
            frames.append(frame)

        frames = tf.stack(frames)
        frames = tf.ensure_shape(frames, (self.num_frames, self.resolution, self.resolution, 3))

        return frames

    def use_classifier(self, sequence_full_path):
        processed_sequence = self.process_video_sequence(sequence_full_path)
        processed_sequence = tf.expand_dims(processed_sequence, 0)
        prediction = self.model.predict(processed_sequence, verbose=0)
        predicted_class = self.class_names[np.argmax(prediction[0])]
        return predicted_class

Classifier = TurnClassifier(model_path = '/content/drive/MyDrive/Graduation Project/Honda/model_weights.h5')

warnings.filterwarnings('ignore')
class FewShotGeminiImageAnalyzer:
    def __init__(self, api_key: str, db_path: str, descriptions_path: str, model_name="gemini-2.5-flash"):
        # Gemini API
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

        # Feature extraction model (ResNet)
        self.feature_extractor = resnet50(pretrained=True)
        self.feature_extractor.fc = torch.nn.Identity()
        self.feature_extractor.eval()

        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        # Load vector DB and descriptions
        self.db_path = db_path
        self.descriptions_path = descriptions_path
        self.frame_embeddings = {}
        self.descriptions = {}
        self.keys = []
        self._load_vector_db()

    def _load_vector_db(self):
        try:
            print(f"[DEBUG] Loading vector DB from: {self.db_path}")
            with open(self.db_path, 'rb') as f:
                data = pickle.load(f)

            print(f"[DEBUG] Loaded data keys: {list(data.keys())}")

            # Handle different possible data structures
            if 'frame_embeddings' in data and 'descriptions' in data:
                self.frame_embeddings = data['frame_embeddings']
                self.descriptions = data['descriptions']
            else:
                # Maybe the structure is different
                print(f"[DEBUG] Data structure: {type(data)}")
                if isinstance(data, dict):
                    # Try to guess the structure
                    for key, value in data.items():
                        print(f"[DEBUG] Key: {key}, Value type: {type(value)}")
                        if isinstance(value, dict) and len(str(key)) > 3:  # Likely a sequence key
                            break

                    # Assume the data itself contains embeddings and descriptions
                    self.frame_embeddings = data

                    # Try to load descriptions from JSON file
                    if os.path.exists(self.descriptions_path):
                        with open(self.descriptions_path, 'r') as f:
                            descriptions_data = json.load(f)
                            self.descriptions = descriptions_data
                    else:
                        print(f"[WARNING] Descriptions file not found: {self.descriptions_path}")
                        self.descriptions = {}

            self.keys = list(self.frame_embeddings.keys())
            print(f"[INFO] Loaded {len(self.frame_embeddings)} sequences from DB")
            print(f"[INFO] Loaded {len(self.descriptions)} descriptions")

            # Debug: Check structure of first few items
            if self.keys:
                first_key = self.keys[0]
                print(f"[DEBUG] First key: {first_key}")
                print(f"[DEBUG] First embedding type: {type(self.frame_embeddings[first_key])}")
                if first_key in self.descriptions:
                    print(f"[DEBUG] First description type: {type(self.descriptions[first_key])}")
                    print(f"[DEBUG] First description keys: {list(self.descriptions[first_key].keys()) if isinstance(self.descriptions[first_key], dict) else 'Not a dict'}")
                else:
                    print(f"[DEBUG] No description found for key: {first_key}")

        except Exception as e:
            print(f"[ERROR] Failed loading DB: {e}")
            import traceback
            traceback.print_exc()

    def _load_frames(self, folder_path: str, num_frames: int = 10) -> List[np.ndarray]:
        files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png'))])
        frames = []
        for f in files[:num_frames]:
            img = cv2.imread(os.path.join(folder_path, f))
            if img is not None:
                frames.append(img)
        while len(frames) < num_frames and frames:
            frames.append(frames[-1])
        return frames

    def _extract_frame_features(self, frame: np.ndarray) -> np.ndarray:
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_tensor = self.transform(frame_rgb).unsqueeze(0)
        with torch.no_grad():
            features = self.feature_extractor(frame_tensor)
        return features.numpy().flatten()

    def _create_sequence_embedding(self, frames: List[np.ndarray]) -> np.ndarray:
        feats = [self._extract_frame_features(f) for f in frames]
        return np.mean(feats, axis=0) if feats else np.zeros(2048)

    def _query_top_similar(self, frames: List[np.ndarray], top_k: int = 4) -> List[Tuple[str, str]]:
        try:
            query_vec = self._create_sequence_embedding(frames)
            similarities = []

            for k in self.keys:
                try:
                    embedding = self.frame_embeddings[k]
                    if isinstance(embedding, np.ndarray):
                        sim = cosine_similarity(query_vec.reshape(1, -1), embedding.reshape(1, -1))[0][0]
                        similarities.append((k, sim))
                    else:
                        print(f"[WARNING] Embedding for key {k} is not numpy array: {type(embedding)}")
                except Exception as e:
                    print(f"[ERROR] Error processing key {k}: {e}")
                    continue

            top = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]

            # Extract descriptions - they are already strings, not dictionaries
            results = []
            for k, _ in top:
                try:
                    if k in self.descriptions:
                        desc_data = self.descriptions[k]
                        if isinstance(desc_data, str):
                            results.append((k, desc_data))
                        else:
                            print(f"[WARNING] Description for key {k} is unexpected type: {type(desc_data)}")
                    else:
                        print(f"[WARNING] No description found for key {k}")
                except Exception as e:
                    print(f"[ERROR] Error extracting description for key {k}: {e}")
                    continue

            return results

        except Exception as e:
            print(f"[ERROR] Error in _query_top_similar: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _build_fewshot_prompt(self, detection_response: str, predicted_class: str, top_examples: List[Tuple[str, str]]) -> str:
        # Updated prompt to match the analyze_with_vector_database format
        prompt = f"""You are an expert in analyzing driving videos. You will be provided with 10 sequential frames (1 frame per 2 seconds) captured from the ego vehicle's camera.

            Your task:
            Describe the driver's actions-just like the provided examples-, focusing on interactions with the road (e.g., yielding, traffic lights, stop signs, turns, tailgating).

            State uncertainties clearly‚Äîif any detail is unclear or not visible, explicitly mention that it cannot be determined. Do not make assumptions.

            Reliable information to use in your analysis:
            Traffic light status: {detection_response}
            Driver's maneuver: {predicted_class} (If the maneuver involves a turn, indicate this appropriately in the sequence.)

            Here are some similar examples to guide your judgment:
            """

        for i, (key, desc) in enumerate(top_examples):
            prompt += f"\nExample {i+1} ({key}):\nAnalysis: {desc}\n---\n"

        prompt += "\nNow analyze this sequence:"

        # ADD PRINT STATEMENT HERE TO SEE THE PROMPT
        print("=" * 80)
        print("[PROMPT] Sending the following prompt to Gemini:")
        print("=" * 80)
        print(prompt)
        print("=" * 80)

        return prompt

    def analyze_frames(self, frames: Union[List[np.ndarray], List[str]], detection_response: str, predicted_class: str) -> Tuple[str, str]:
        try:
            if len(frames) < 2:
                return "Error: Need at least 2 frames", "Insufficient input"

            # Convert input to numpy arrays if they are file paths
            numpy_frames = []

            for i, frame in enumerate(frames):
                if isinstance(frame, str):
                    # It's a file path, load the image
                    try:
                        img = cv2.imread(frame)
                        if img is None:
                            print(f"[WARNING] Could not load image from path: {frame}")
                            continue
                        numpy_frames.append(img)
                    except Exception as e:
                        print(f"[ERROR] Failed to load frame from path {frame}: {e}")
                        continue
                elif isinstance(frame, np.ndarray):
                    # It's already a numpy array
                    numpy_frames.append(frame)
                else:
                    print(f"[WARNING] Frame {i} is invalid type: {type(frame)}")
                    continue

            if len(numpy_frames) < 2:
                return "Error: Need at least 2 valid frames after processing", "Insufficient valid input"

            # Use numpy_frames for similarity search
            top_examples = self._query_top_similar(numpy_frames)

            # ADD PRINT STATEMENT HERE TO SEE THE SIMILAR EXAMPLES FOUND
            print(f"[DEBUG] Found {len(top_examples)} similar examples:")
            for i, (key, desc) in enumerate(top_examples):
                print(f"  Example {i+1}: {key}")
                print(f"  Description: {desc[:100]}..." if len(desc) > 100 else f"  Description: {desc}")
                print()

            prompt = self._build_fewshot_prompt(detection_response, predicted_class, top_examples)

            # Create temporary directory for Gemini API
            temp_dir = tempfile.mkdtemp()
            uploaded_parts = []

            try:
                for i, frame in enumerate(numpy_frames[:10]):
                    if frame is None or not isinstance(frame, np.ndarray):
                        print(f"[WARNING] Frame {i} is invalid: {type(frame)}")
                        continue

                    try:
                        # Ensure frame is in correct format for PIL
                        if len(frame.shape) == 3 and frame.shape[2] == 3:
                            # OpenCV uses BGR, PIL expects RGB
                            if isinstance(frames[0], str):  # If original input was file paths, assume BGR from cv2.imread
                                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            else:  # If original input was numpy arrays, assume they're already RGB
                                frame_rgb = frame
                        else:
                            print(f"[WARNING] Frame {i} has unexpected shape: {frame.shape}")
                            continue

                        img = Image.fromarray(frame_rgb.astype("uint8"))

                    except Exception as e:
                        print(f"[ERROR] Failed converting frame {i}: {e}")
                        # Try alternative conversion
                        try:
                            img = Image.fromarray(frame.astype("uint8"))
                        except Exception as e2:
                            print(f"[ERROR] Alternative conversion also failed for frame {i}: {e2}")
                            continue

                    # Save image and prepare for Gemini API
                    path = os.path.join(temp_dir, f"frame_{i:04d}.jpg")
                    img.save(path)

                    with open(path, 'rb') as f:
                        # Try different ways to create the Part object
                        try:
                            # Method 1: from_bytes with just data
                            part = genai.types.Part.from_bytes(f.read())
                            uploaded_parts.append(part)
                        except Exception as e1:
                            try:
                                # Method 2: Direct Part creation
                                f.seek(0)  # Reset file pointer
                                part = genai.types.Part(data=f.read(), mime_type='image/jpeg')
                                uploaded_parts.append(part)
                            except Exception as e2:
                                try:
                                    # Method 3: Using inline_data
                                    f.seek(0)  # Reset file pointer
                                    part = genai.types.Part(
                                        inline_data=genai.types.Blob(
                                            data=f.read(),
                                            mime_type='image/jpeg'
                                        )
                                    )
                                    uploaded_parts.append(part)
                                except Exception as e3:
                                    print(f"[ERROR] All Part creation methods failed for frame {i}")
                                    print(f"Method 1 error: {e1}")
                                    print(f"Method 2 error: {e2}")
                                    print(f"Method 3 error: {e3}")
                                    continue

                if not uploaded_parts:
                    return "Error: No valid frames could be processed for Gemini API", "No valid frames"

                # ADD PRINT STATEMENT HERE TO SEE WHAT'S BEING SENT TO GEMINI
                print(f"[DEBUG] Sending to Gemini API:")
                print(f"  - Model: {self.model_name}")
                print(f"  - Number of uploaded image parts: {len(uploaded_parts)}")
                print(f"  - Content structure: [prompt_text] + {len(uploaded_parts)} image parts")

                # Gemini API call
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=[prompt] + uploaded_parts
                )
                original_response = response.text

                # ADD PRINT STATEMENT HERE TO SEE GEMINI'S RESPONSE
                print("=" * 80)
                print("[RESPONSE] Gemini's response:")
                print("=" * 80)
                print(original_response)
                print("=" * 80)

            except Exception as e:
                original_response = f"Gemini Error: {str(e)}"
                import traceback
                print(f"[ERROR] Full traceback: {traceback.format_exc()}")
            finally:
                # Clean up temporary directory
                try:
                    shutil.rmtree(temp_dir)
                except Exception as e:
                    print(f"[WARNING] Failed to clean up temp directory: {e}")

            return original_response

        except Exception as e:
            print(f"[ERROR] Error in analyze_frames: {e}")
            import traceback
            traceback.print_exc()
            return f"Error: {str(e)}", "Error in analysis"

vector_db_path = "/content/drive/MyDrive/Graduation Project/Qwen/frame_sequence_vector_db.pkl"
descriptions_json_path = "/content/drive/MyDrive/Graduation Project/Qwen/VectorDatabase.json"

fewshot_analyzer = FewShotGeminiImageAnalyzer(
    api_key="AIzaSyDbgEY7ckIN4_NJA4sSbeYSEEEL59nfcLY",
    db_path=vector_db_path,
    descriptions_path=descriptions_json_path
)

class DriverAdviceRetriever:
    def __init__(self, json_path, chroma_path="./chroma_data"):
        self.json_path = json_path
        self.chroma_path = chroma_path
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.collection_name = "advice_collection"

        self.descriptions = []
        self.advices = []
        self.ids = []

        self.collection = None
        self.chroma_client = chromadb.PersistentClient(path=self.chroma_path)

        self._load_data()
        self._initialize_chroma()

    def _load_data(self):
        try:
            with open(self.json_path, "r") as f:
                data = json.load(f)

            self.descriptions = [item["behaviour"] for item in data.values()]
            self.advices = [item["advice"] for item in data.values()]
            self.ids = list(data.keys())

            print(f"Loaded {len(self.descriptions)} items from JSON.")
        except Exception as e:
            print("Failed to load data:", e)
            traceback.print_exc()

    def _initialize_chroma(self):
        try:
            # Try to delete existing collection
            try:
                self.chroma_client.delete_collection(self.collection_name)
                print("Deleted existing Chroma collection.")
            except:
                print("No existing Chroma collection to delete.")

            self.collection = self.chroma_client.create_collection(name=self.collection_name)

            # Embed and add to Chroma
            description_embeddings = self.model.encode(self.descriptions).tolist()
            self.collection.add(
                documents=self.advices,
                embeddings=description_embeddings,
                metadatas=[{"description": desc} for desc in self.descriptions],
                ids=self.ids
            )
            print(f"Added {len(self.ids)} items to Chroma collection.")
        except Exception as e:
            print("Failed to initialize Chroma DB:", e)
            traceback.print_exc()

    def query_advice(self, user_input, top_k=3):
        try:
            query_embedding = self.model.encode(user_input).tolist()
            result = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )

            print(f"\nQuery: {user_input}")
            print("Top matches:")
            for i in range(len(result['documents'][0])):
                print(f"{i+1}. Description: {result['metadatas'][0][i]['description']}")
                print(f"   Advice: {result['documents'][0][i]}")
                print(f"   Distance: {result['distances'][0][i]}\n")

            return result['documents'][0][0]  # Return top advice
        except Exception as e:
            print("Query failed:", e)
            traceback.print_exc()
            return None

# Example usage:
if __name__ == "__main__":
    retriever = DriverAdviceRetriever("/content/drive/MyDrive/Graduation Project/Qwen/VectorDatabase.json")

class ScenarioEvaluator:
    def __init__(self, api_key: str, model_name="gemini-2.5-flash"):
        from google import genai
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

    def build_prompt(self, description: str, advice: str):
        return f"""
        Your task is to evaluate whether the driver behaved correctly or not, based on the description and the advice.

        From the following list of scenarios:
        Traffic Light, Pedestrians, Stop Sign, High Speed, Safe distance, Headlights

        1. Identify which scenarios are explicitly mentioned in the *description*.
        2. For each of these scenarios, assess whether the driver behaved correctly by comparing the description with the advice:
          - Respond with *Incorrect* if the description contradicts the advice in that scenario.
          - Respond with *Correct* if the description aligns with the advice, or if the description does not contradict it.
          - Respond with *No Scenarios* if none is mentioned.

        Output only the scenarios mentioned in the description, each followed by a colon and either *Correct* or *Incorrect*, one per line. Do not include any extra text.

        Format:
        ScenarioName: Correct
        ScenarioName: Incorrect

        description: {description}
        advice: {advice}
        """

    def evaluate(self, description: str, advice: str):
        prompt = self.build_prompt(description, advice)
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[prompt]
            )
            return response.text
        except Exception as e:
         return f"Scenario Evaluation Error: {str(e)}"

evaluator = ScenarioEvaluator(api_key="AIzaSyDbgEY7ckIN4_NJA4sSbeYSEEEL59nfcLY")

def extract_frames_every_n_seconds(video_path, interval=2):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    interval_frames = int(fps * interval)

    frames = []
    count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval_frames == 0:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
        count += 1

    cap.release()
    return frames

import gradio as gr
from collections import defaultdict
import tempfile
import shutil
import os

def process_with_individual_updates(video):
    """Process video and update each textbox individually as soon as its result is ready"""

    def update_single_component(component_index, value):
        """Helper to create update for a single specific component"""
        updates = [gr.update() for _ in range(7)]  # 7 total outputs
        updates[component_index] = gr.update(value=value)
        return updates

    try:
        temp_dir = tempfile.mkdtemp()
        video_path = os.path.join(temp_dir, "input_video.mp4")
        shutil.copy(video, video_path)

        all_frames = extract_frames_every_n_seconds(video_path, interval=2)

        # Initialize accumulators
        all_detections, all_classes = "", ""
        all_gemini, all_advice, all_eval = "", "", ""
        scenario_counts = defaultdict(int)
        scenario_correct = defaultdict(int)

        for i in range(0, len(all_frames), 10):
            batch_frames = all_frames[i:i+10]
            if len(batch_frames) < 10:
                continue

            batch_num = i//10 + 1
            header = f"----- BATCH {batch_num} -----\n"

            try:
                # Step 1: Process frames and get detection results
                # Note: You'll need to modify process_uploaded_frames to return individual steps
                # For now, assuming we can call individual processing steps

                # Detection step
                detection_response = "\n".join(detector.filter_detections(
                    detector.detect_in_sequence(temp_dir_for_batch)  # You'd need to create temp dir for this batch
                ))
                all_detections += f"{header}{detection_response}\n\n"
                yield update_single_component(0, all_detections.strip())  # Update Detection immediately

                # Classification step
                predicted_class = Classifier.use_classifier(temp_dir_for_batch)
                all_classes += f"{header}{predicted_class}\n\n"
                yield update_single_component(1, all_classes.strip())  # Update Class immediately

                # Gemini Analysis step
                try:
                    original_response = fewshot_analyzer.analyze_frames(
                        frame_paths, detection_response, predicted_class
                    )
                except Exception as e:
                    original_response = f"Gemini API Error: {str(e)}"

                all_gemini += f"{header}{original_response}\n\n"
                yield update_single_component(2, all_gemini.strip())  # Update Gemini immediately

                # Advice Retrieval step
                if (original_response and
                    original_response != "Overall driver behavior not found in response" and
                    not original_response.startswith("Error:")):
                    try:
                        top_advice = retriever.query_advice(original_response)
                    except Exception as e:
                        top_advice = f"Error getting advice: {str(e)}"
                else:
                    top_advice = "No advice available ‚Äî overall behavior not extracted."

                all_advice += f"{header}{top_advice}\n\n"
                yield update_single_component(3, all_advice.strip())  # Update Advice immediately

                # Scenario Evaluation step
                if top_advice.startswith("Error") or original_response.startswith("Error"):
                    scenario_evaluation = "Could not evaluate scenarios due to missing behavior or advice."
                else:
                    scenario_evaluation = evaluator.evaluate(original_response, top_advice)

                all_eval += f"{header}{scenario_evaluation}\n\n"
                yield update_single_component(4, all_eval.strip())  # Update Evaluation immediately

            except Exception as e:
                error_msg = str(e)
                all_detections += f"{header}Error: {error_msg}\n\n"
                all_classes += f"{header}Error: {error_msg}\n\n"
                all_gemini += f"{header}Error: {error_msg}\n\n"
                all_advice += f"{header}Error: {error_msg}\n\n"
                all_eval += f"{header}Error: {error_msg}\n\n"

                # Update all with error
                yield update_single_component(0, all_detections.strip())
                yield update_single_component(1, all_classes.strip())
                yield update_single_component(2, all_gemini.strip())
                yield update_single_component(3, all_advice.strip())
                yield update_single_component(4, all_eval.strip())

            # Process scenario evaluation for scoring (after each batch)
            if scenario_evaluation and "No Scenarios" not in scenario_evaluation and not scenario_evaluation.startswith("Error"):
                for line in scenario_evaluation.strip().splitlines():
                    if ':' in line:
                        scenario, result = [part.strip() for part in line.split(':', 1)]
                        if scenario in ["Traffic Light", "Pedestrians", "Stop Sign", "High Speed", "Safe distance", "Headlights"]:
                            scenario_counts[scenario] += 1
                            if result.lower() == "correct":
                                scenario_correct[scenario] += 1

        # Calculate and display final scores
        final_scores = "\nüìä Driver Behavior Evaluation ‚Äî Final Scores:\n"
        for scenario in ["Traffic Light", "Pedestrians", "Stop Sign", "High Speed", "Safe distance", "Headlights"]:
            total = scenario_counts[scenario]
            correct = scenario_correct[scenario]
            score = f"{(correct / total) * 100:.2f}%" if total > 0 else "N/A"
            final_scores += f"{scenario}: {correct}/{total} correct ‚Üí Score: {score}\n"

        yield update_single_component(5, final_scores.strip())  # Update Final Scores

        # Calculate and display overall score
        total_all = sum(scenario_counts.values())
        correct_all = sum(scenario_correct.values())
        overall_score = f"{(correct_all / total_all) * 100:.2f}%" if total_all > 0 else "N/A"
        overall_score_text = f"\nüèÅ Final Driver Score: {correct_all}/{total_all} correct ‚Üí Score: {overall_score}"

        yield update_single_component(6, overall_score_text.strip())  # Update Overall Score

        shutil.rmtree(temp_dir)

    except Exception as e:
        error_text = str(e)
        for i in range(7):
            yield update_single_component(i, error_text)


# Alternative approach: Modify your existing process_uploaded_frames to be a generator
def process_uploaded_frames_step_by_step(*frames):
    """Modified version that yields each step individually"""
    try:
        temp_dir = tempfile.mkdtemp()
        frame_paths = []

        for i, frame in enumerate(frames):
            if frame is not None:
                image = Image.fromarray(frame.astype("uint8"))
                path = os.path.join(temp_dir, f"frame_{i:04d}.jpg")
                image.save(path)
                frame_paths.append(path)

        # Step 1: Detector
        detection_response = "\n".join(detector.filter_detections(
            detector.detect_in_sequence(temp_dir)
        ))
        yield ("detection", detection_response)

        # Step 2: Classifier
        predicted_class = Classifier.use_classifier(temp_dir)
        yield ("classification", predicted_class)

        # Step 3: Gemini Analyzer
        try:
            original_response = fewshot_analyzer.analyze_frames(
                frame_paths, detection_response, predicted_class
            )
        except Exception as e:
            original_response = f"Gemini API Error: {str(e)}"
        yield ("gemini", original_response)

        # Step 4: Advice Retriever
        if (original_response and
            original_response != "Overall driver behavior not found in response" and
            not original_response.startswith("Error:")):
            try:
                top_advice = retriever.query_advice(original_response)
            except Exception as e:
                top_advice = f"Error getting advice: {str(e)}"
        else:
            top_advice = "No advice available ‚Äî overall behavior not extracted."
        yield ("advice", top_advice)

        # Step 5: Scenario Evaluator
        if top_advice.startswith("Error") or original_response.startswith("Error"):
            scenario_evaluation = "Could not evaluate scenarios due to missing behavior or advice."
        else:
            scenario_evaluation = evaluator.evaluate(original_response, top_advice)
        yield ("evaluation", scenario_evaluation)

        shutil.rmtree(temp_dir)

    except Exception as e:
        yield ("error", str(e))


def process_with_step_by_step_updates(video):
    """Process video using the step-by-step approach"""

    def update_single_component(component_index, value):
        updates = [gr.update() for _ in range(7)]
        updates[component_index] = gr.update(value=value)
        return updates

    try:
        temp_dir = tempfile.mkdtemp()
        video_path = os.path.join(temp_dir, "input_video.mp4")
        shutil.copy(video, video_path)

        all_frames = extract_frames_every_n_seconds(video_path, interval=2)

        # Initialize accumulators
        all_detections, all_classes = "", ""
        all_gemini, all_advice, all_eval = "", "", ""
        scenario_counts = defaultdict(int)
        scenario_correct = defaultdict(int)

        for i in range(0, len(all_frames), 10):
            batch_frames = all_frames[i:i+10]
            if len(batch_frames) < 10:
                continue

            batch_num = i//10 + 1
            header = f"----- BATCH {batch_num} -----\n"

            # Process each step individually
            for step_type, result in process_uploaded_frames_step_by_step(*batch_frames):
                if step_type == "detection":
                    all_detections += f"{header}{result}\n\n"
                    yield update_single_component(0, all_detections.strip())

                elif step_type == "classification":
                    all_classes += f"{header}{result}\n\n"
                    yield update_single_component(1, all_classes.strip())

                elif step_type == "gemini":
                    all_gemini += f"{header}{result}\n\n"
                    yield update_single_component(2, all_gemini.strip())

                elif step_type == "advice":
                    all_advice += f"{header}{result}\n\n"
                    yield update_single_component(3, all_advice.strip())

                elif step_type == "evaluation":
                    all_eval += f"{header}{result}\n\n"
                    yield update_single_component(4, all_eval.strip())

                    # Process scenario scoring
                    if result and "No Scenarios" not in result and not result.startswith("Error"):
                        for line in result.strip().splitlines():
                            if ':' in line:
                                scenario, eval_result = [part.strip() for part in line.split(':', 1)]
                                if scenario in ["Traffic Light", "Pedestrians", "Stop Sign", "High Speed", "Safe distance", "Headlights"]:
                                    scenario_counts[scenario] += 1
                                    if eval_result.lower() == "correct":
                                        scenario_correct[scenario] += 1

                elif step_type == "error":
                    error_text = f"{header}Error: {result}\n\n"
                    all_detections += error_text
                    all_classes += error_text
                    all_gemini += error_text
                    all_advice += error_text
                    all_eval += error_text

                    for j in range(5):
                        yield update_single_component(j, [all_detections, all_classes, all_gemini, all_advice, all_eval][j].strip())

        # Final scores and overall score (same as before)
        final_scores = "\nüìä Driver Behavior Evaluation ‚Äî Final Scores:\n"
        for scenario in ["Traffic Light", "Pedestrians", "Stop Sign", "High Speed", "Safe distance", "Headlights"]:
            total = scenario_counts[scenario]
            correct = scenario_correct[scenario]
            score = f"{(correct / total) * 100:.2f}%" if total > 0 else "N/A"
            final_scores += f"{scenario}: {correct}/{total} correct ‚Üí Score: {score}\n"

        yield update_single_component(5, final_scores.strip())

        total_all = sum(scenario_counts.values())
        correct_all = sum(scenario_correct.values())
        overall_score = f"{(correct_all / total_all) * 100:.2f}%" if total_all > 0 else "N/A"
        overall_score_text = f"\nüèÅ Final Driver Score: {correct_all}/{total_all} correct ‚Üí Score: {overall_score}"

        yield update_single_component(6, overall_score_text.strip())

        shutil.rmtree(temp_dir)

    except Exception as e:
        error_text = str(e)
        for i in range(7):
            yield update_single_component(i, error_text)


# Gradio interface (same as before)
with gr.Blocks(title="Driver Behavior Analysis üöó") as demo:
    gr.Markdown("## <div style='text-align: center;'>üöó Driver Behavior From Video</div>")
    gr.Markdown("### Upload a Driving Video (analyzed step-by-step)")

    video_input = gr.Video(label="Upload Driving Video", format="mp4")
    analyze_button = gr.Button("Run Analysis")

    with gr.Column():
        outputs = [
            gr.Textbox(label="Filtered Detections", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Predicted Class", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Gemini JSON Output", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Top Driving Advice", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Scenario Evaluation", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Final Scenario Scores", lines=10, max_lines=500, show_copy_button=True),
            gr.Textbox(label="Overall Driver Score", lines=2, show_copy_button=True)
        ]

    analyze_button.click(
        fn=process_with_step_by_step_updates,  # Use the step-by-step version
        inputs=[video_input],
        outputs=outputs
    )

demo.launch(debug=True)







